{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OpenAI's GPT 3.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tiktoken"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.422373100Z",
     "start_time": "2023-05-23T20:16:26.310581700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "path_train = 'data/sts-train.csv'\n",
    "path_dev = 'data/sts-dev.csv'\n",
    "path_test = 'data/sts-test.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.522292100Z",
     "start_time": "2023-05-23T20:16:26.330227100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "columns=['genre', 'file', 'year', 'index', 'score', 'sentence1', 'sentence2']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.533508600Z",
     "start_time": "2023-05-23T20:16:26.345003400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path_train, sep='\\t', usecols=range(7), header=None, quoting=csv.QUOTE_NONE, names=columns, encoding='UTF-8')\n",
    "df_dev = pd.read_csv(path_dev, sep='\\t', usecols=range(7), header=None, quoting=csv.QUOTE_NONE, names=columns, encoding='UTF-8')\n",
    "df_test = pd.read_csv(path_test, sep='\\t', usecols=range(7), header=None, quoting=csv.QUOTE_NONE, names=columns, encoding='UTF-8')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.533508600Z",
     "start_time": "2023-05-23T20:16:26.360508700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def pre_processing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['genre'] = df['genre'].replace('main-', '', regex=True)\n",
    "    df['genre'] = df['genre'].replace('forum', 'forums')\n",
    "    df['year'] = df['year'].replace(r'\\D', '', regex=True)\n",
    "    df['score'] = MinMaxScaler().fit_transform(df[['score']])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.533508600Z",
     "start_time": "2023-05-23T20:16:26.397630300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "df_train = pre_processing(df_train)\n",
    "df_dev = pre_processing(df_dev)\n",
    "df_test = pre_processing(df_test)\n",
    "df_train_dev = pd.concat([df_train, df_dev]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.533508600Z",
     "start_time": "2023-05-23T20:16:26.409995Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def evaluate(scores: np.ndarray, amount=len(df_test)) -> None:\n",
    "    actual_scores = df_test['score'][0:amount].to_numpy()\n",
    "    print(f\"R2: {r2_score(actual_scores, scores):.5f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(actual_scores, scores):.5f}\")\n",
    "    print(f\"RMSE: {mean_squared_error(actual_scores, scores, squared=False):.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.533508600Z",
     "start_time": "2023-05-23T20:16:26.443896Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Asking ChatGPT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "openai.api_key  = \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.533508600Z",
     "start_time": "2023-05-23T20:16:26.460751800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "system_content = \"\"\"\n",
    "Your task is to compute the similarity of multiple pair of sentences.\n",
    "The similarity is a real number that takes range from 0 to 5.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.533508600Z",
     "start_time": "2023-05-23T20:16:26.476376500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "score_guidelines = \"\"\"\n",
    "To determine the score, follow the guidelines delimited by triple backticks. Note that they are not labels, but guidelines. You can answer with any score as long as it is a real number and takes range from 0.0 to 5.0.\n",
    "```\n",
    "- Score 5.0: the two sentences are completely equivalent, as they mean the same thing.\n",
    "- Score 4.0: the two sentences are mostly equivalent, but some unimportant details differ.\n",
    "- Score 3.0: the two sentences are roughly equivalent, but some important information differs/missing.\n",
    "- Score 2.0: the two sentences are not equivalent, but share some details.\n",
    "- Score 1.0: the two sentences are not equivalent, but are on the same topic.\n",
    "- Score 0.0: the the two sentences are completely dissimilar.\n",
    "```\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.533508600Z",
     "start_time": "2023-05-23T20:16:26.496293600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "input_format = \"\"\"\n",
    "You will receive the multiple pair of sentences in the following format, delimited by triple backticks:\n",
    "```\n",
    "[Pair 1]\n",
    "- \"First sentence of the first pair.\"\n",
    "- \"Second sentence of the first pair.\"\n",
    "\n",
    "[Pair 2]\n",
    "- \"First sentence of the second pair.\"\n",
    "- \"Second sentence of the second pair.\"\n",
    "\n",
    "[Pair 3]\n",
    "- \"First sentence of the third pair.\"\n",
    "- \"Second sentence of the third pair.\"\n",
    "```\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.533508600Z",
     "start_time": "2023-05-23T20:16:26.523297100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "few_shots_examples = \"\"\"\n",
    "The following are few-shot sample pairs of sentences and their similarity score:\n",
    "\n",
    "Example 1:\n",
    "- \"A cat is playing a piano.\"\n",
    "- \"A man is playing a guitar.\"\n",
    "Score: 0.6\n",
    "\n",
    "Example 2:\n",
    "- \"Runners race around a track.\"\n",
    "- \"Runners compete in a race.\"\n",
    "Score: 3.2\n",
    "\n",
    "Example 3:\n",
    "- \"A person is slicing a tomato.\"\n",
    "- \"A person is slicing some meat.\"\n",
    "Score: 1.75\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.540058200Z",
     "start_time": "2023-05-23T20:16:26.524501900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "output_format = \"\"\"\n",
    "Your answer must contain a a list of scores that has one element for each pair received. The first element of the list must be the similarity score of the first pair of sentences, the second element must be the similarity score of the second pair of sentences, and so on and so forth.\n",
    "\n",
    "An example of an answer is delimited in triple backticks below:\n",
    "```[ 2.7, 3.2, 1.3 ]```\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.582944300Z",
     "start_time": "2023-05-23T20:16:26.540058200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def concat_sentences(start: int, amount: int = 15) -> str:\n",
    "    message = \"The following are the sentences you need to compute the similarity score for:\"\n",
    "    for i, j in enumerate(range(start, start + amount)):\n",
    "        message += f\"\"\"\n",
    "[Pair {i + 1}]\n",
    "- \"{df_test['sentence1'][j]}\"\n",
    "- \"{df_test['sentence2'][j]}\"\n",
    "\"\"\"\n",
    "    return message"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.583944800Z",
     "start_time": "2023-05-23T20:16:26.560943600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT-3.5 can handle at most 4096 tokens in a single request.\n",
    "We need to use a tokenizer to make sure that the prompt does not exceed the limit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.591944300Z",
     "start_time": "2023-05-23T20:16:26.573944100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.605943800Z",
     "start_time": "2023-05-23T20:16:26.591944300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def perform_request(sentences: str) -> list[float]:\n",
    "    # Concatenate all the strings inside message dictionary\n",
    "    prompt = system_content + score_guidelines + input_format + few_shots_examples + output_format + sentences\n",
    "\n",
    "    # Check the number of tokens in the message\n",
    "    if  len(tokenizer.encode(prompt)) > 3900:\n",
    "        raise Exception(\"The number of tokens in the message exceeds the limit.\")\n",
    "\n",
    "    # Perform the request and return the response\n",
    "    response = get_completion(prompt)\n",
    "    return literal_eval(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.638780500Z",
     "start_time": "2023-05-23T20:16:26.607945800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "pred_scores = list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T20:16:26.639792300Z",
     "start_time": "2023-05-23T20:16:26.621338400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/13 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f998db3998b41d895eb61338556e4c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of sentences from 1171 to 1185: [3.5, 4.5, 3.5, 4.5, 1.0, 1.0, 4.5, 2.5, 4.0, 0.5, 1.0, 4.5, 2.0, 2.5, 1.0]\n",
      "Similarity of sentences from 1186 to 1200: [4.0, 4.5, 4.0, 3.5, 4.5, 3.5, 4.0, 2.5, 2.0, 1.5, 1.0, 4.5, 4.5, 3.5, 4.0]\n",
      "Similarity of sentences from 1201 to 1215: [3.5, 2.5, 4.5, 4.5, 4.8, 4.0, 5.0, 3.0, 1.5, 2.8, 3.5, 5.0, 4.5, 2.0, 4.5]\n",
      "Similarity of sentences from 1216 to 1230: [4.5, 2.5, 3.5, 3.8, 2.2, 2.5, 3.8, 3.5, 3.2, 2.8, 3.5, 4.5, 4.5, 4.5, 3.5]\n",
      "Similarity of sentences from 1231 to 1245: [4.0, 3.5, 3.5, 4.2, 2.5, 3.8, 3.0, 2.0, 0.5, 1.5, 2.0, 2.8, 3.5, 2.5, 1.0]\n",
      "Similarity of sentences from 1246 to 1260: [4.5, 4.2, 1.2, 0.8, 4.5, 4.5, 4.0, 4.5, 1.5, 0.5, 4.8, 2.5, 4.8, 3.5, 4.5]\n",
      "Similarity of sentences from 1261 to 1275: [3.5, 2.8, 4.5, 1.2, 3.8, 3.5, 4.5, 4.0, 1.0, 1.5, 3.5, 4.0, 2.0, 4.5, 4.2]\n",
      "Similarity of sentences from 1276 to 1290: [2.5, 1.8, 4.5, 1.2, 4.2, 1.0, 1.5, 2.5, 4.0, 1.5, 4.5, 4.5, 1.5, 1.0, 2.5]\n",
      "Similarity of sentences from 1291 to 1305: [4.5, 1.5, 4.0, 4.5, 3.2, 1.0, 4.0, 1.5, 1.75, 4.5, 3.5, 2.5, 2.0, 2.5, 2.5]\n",
      "Similarity of sentences from 1306 to 1320: [1.5, 3.8, 2.5, 3.7, 3.2, 4.0, 4.8, 2.0, 4.2, 2.8, 4.5, 4.5, 4.5, 2.0, 0.5]\n",
      "Similarity of sentences from 1321 to 1335: [4.5, 2.8, 4.5, 3.5, 4.9, 4.0, 4.8, 3.2, 1.5, 4.8, 2.5, 1.0, 2.5, 2.0, 3.5]\n",
      "Similarity of sentences from 1336 to 1350: [4.5, 4.2, 4.5, 1.2, 3.8, 4.5, 4.8, 2.5, 1.5, 1.0, 2.5, 4.8, 4.0, 4.8, 2.5]\n",
      "Similarity of sentences from 1351 to 1365: [5.0, 4.5, 4.5, 4.0, 3.5, 3.5, 2.0, 4.5, 2.5, 2.0, 1.0, 2.5, 2.0, 4.5, 4.0]\n"
     ]
    }
   ],
   "source": [
    "start_sentence = 0\n",
    "n_sentences = 1365\n",
    "sentences_per_request = 15\n",
    "\n",
    "for i in trange(start_sentence, n_sentences, sentences_per_request):\n",
    "    message = concat_sentences(i, amount=sentences_per_request)\n",
    "    response = perform_request(message)\n",
    "    print(f\"Similarity of sentences from {i + 1} to {i + sentences_per_request}: {response}\")\n",
    "    if len(response) != sentences_per_request:\n",
    "        raise Exception(\"The number of scores returned is not equal to the number of sentences.\")\n",
    "    pred_scores = pred_scores + response\n",
    "    time.sleep(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T21:05:53.412772100Z",
     "start_time": "2023-05-23T21:02:43.046651500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "(15, 1365)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response), len(pred_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T21:06:10.713203700Z",
     "start_time": "2023-05-23T21:06:10.697064900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.50762\n",
      "MAE: 0.16446\n",
      "RMSE: 0.21347\n"
     ]
    }
   ],
   "source": [
    "evaluate(np.array(pred_scores) / 5, amount=len(pred_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T21:06:12.548160100Z",
     "start_time": "2023-05-23T21:06:12.523795700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
